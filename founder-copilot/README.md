# Founder Copilot

An AI-powered assistant for startup founders, built with OpenAI's Assistant API. Get personalized advice on fundraising, B2B strategies, and startup best practices using a knowledge base of curated startup resources.

## Features

- ü§ñ **AI Assistant** - Powered by GPT-4 with retrieval-augmented generation (RAG)
- üìö **Knowledge Base** - Vector store containing startup resources (YC advice, checklists, scenarios)
- üí¨ **Modern Chat Interface** - Beautiful, responsive web UI with structured responses
- üîç **File Search** - Automatic retrieval of relevant information from knowledge base
- üìù **Source Citations** - Automatic extraction and display of source files with quotes
- üßÆ **Code Interpreter** - Python code execution for founder analytics, calculations, and visualizations
- üìà **Chart Display** - Automatic extraction and display of charts/visualizations generated by code_interpreter
- üìé **File Upload** - Upload CSV, Excel, or other files for analysis and visualization
- üìä **Metrics Dashboard** - Track token usage, latency (P95), and request statistics
- üö¶ **Rate Limiting** - Redis-based rate limiting to protect API endpoints
- üê≥ **Docker Support** - Easy deployment with Docker Compose

## OpenAI API Usage

This project uses **OpenAI's Assistants API (Beta)** to create an intelligent assistant with retrieval capabilities. Here's how each API component is used:

### 1. Vector Stores API (Beta)

**Purpose**: Store and manage knowledge base files for retrieval

**Usage**:
- **`vector_stores.create()`** - Creates a new vector store to hold knowledge base files
- **`vector_stores.file_batches.upload_and_poll()`** - Uploads multiple files in a batch and polls for completion
  - Supports `.md`, `.txt`, and `.json` files
  - Automatically chunks and indexes files for semantic search

**Implementation**:
```python
# Create vector store
vs = client.beta.vector_stores.create(name="founder_copilot_knowledge")

# Upload files in batch
batch = client.beta.vector_stores.file_batches.upload_and_poll(
    vector_store_id=vs.id,
    files=[open("data/file1.md", "rb"), open("data/file2.md", "rb")]
)
```

### 2. Assistants API (Beta)

**Purpose**: Create and manage AI assistants with tools and knowledge

**Usage**:
- **`assistants.create()`** - Creates an assistant with:
  - Custom instructions (YC-style startup advisor persona)
  - Model selection (configurable via `OPENAI_MODEL`, defaults to `gpt-4.1`)
  - File search tool for retrieval from vector stores
  - Code interpreter tool for analytics and calculations
  - Vector store integration for knowledge base access

**Implementation**:
```python
assistant = client.beta.assistants.create(
    name="FounderCopilot",
    model="gpt-4.1",
    instructions="You are a YC-style startup advisor...",
    tools=[
        {"type": "file_search"},
        {"type": "code_interpreter"}  # For founder analytics
    ],
    tool_resources={
        "file_search": {
            "vector_store_ids": [vector_store_id]
        }
    }
)
```

### 3. Threads API (Beta)

**Purpose**: Manage conversation threads for multi-turn dialogues

**Usage**:
- **`threads.create()`** - Creates a new conversation thread
- **`threads.messages.create()`** - Adds messages to a thread (user or assistant)
- **`threads.runs.create()`** - Executes the assistant on a thread
- **`threads.runs.retrieve()`** - Polls run status until completion
- **`threads.messages.list()`** - Retrieves conversation history

**Implementation**:
```python
# Create thread
thread = client.beta.threads.create()

# Add user message
client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="How do I raise pre-seed funding?"
)

# Run assistant
run = client.beta.threads.runs.create(
    thread_id=thread.id,
    assistant_id=assistant.id
)

# Poll for completion
while run.status not in ("completed", "failed"):
    run = client.beta.threads.runs.retrieve(
        thread_id=thread.id,
        run_id=run.id
    )
```

### 4. File Search Tool

**Purpose**: Enable the assistant to retrieve relevant information from the vector store

**How it works**:
- When a user asks a question, the assistant automatically uses the `file_search` tool
- The tool searches the vector store for semantically similar content
- Relevant snippets are retrieved and included in the assistant's response
- The assistant cites sources when referencing knowledge base content
- Source citations are automatically extracted from message annotations

**Benefits**:
- Up-to-date information without retraining models
- Ability to add/update knowledge base files without changing the assistant
- Automatic relevance ranking and retrieval
- Citation of sources for transparency
- Structured output with answer, bullets, and source references

### 5. Code Interpreter Tool

**Purpose**: Enable the assistant to run Python code for founder analytics, calculations, and data visualization

**How it works**:
- The assistant automatically uses `code_interpreter` when questions involve:
  - Financial calculations (burn rate, runway, growth rates, etc.)
  - Data analysis and statistical computations
  - Creating charts and visualizations
  - Modeling scenarios and what-if analyses
  - Performing complex calculations that require code execution

**Use Cases**:
- **Financial Metrics**: Calculate burn rate, runway, customer acquisition cost (CAC), lifetime value (LTV), etc.
- **Growth Analysis**: Analyze growth rates, cohort analysis, conversion funnels
- **Projections**: Model revenue projections, funding scenarios, hiring plans
- **Visualizations**: Create charts and graphs for metrics, trends, and comparisons
- **Statistical Analysis**: Perform regression analysis, A/B test calculations, correlation analysis
- **File Analysis**: Upload CSV/Excel files and analyze data, create visualizations, and generate insights

**Example Questions**:
- "If I have $500K in the bank and burn $50K/month, how long is my runway?"
- "Calculate my CAC:LTV ratio if CAC is $100 and LTV is $500"
- "Create a chart showing revenue growth over the last 6 months"
- "What's my break-even point if fixed costs are $20K/month and margin is 60%?"
- "Here's kpi_tracker.csv ‚Äî visualize our Q1 progress vs targets"

**File Upload Support**:
- Upload CSV, Excel, JSON, or text files via the web UI
- Files are automatically attached to your message
- The assistant can read, analyze, and visualize data from uploaded files
- Charts and visualizations generated from file data are automatically displayed in the chat

**Benefits**:
- Accurate calculations for financial and business metrics
- Dynamic visualizations and charts that are automatically displayed in the UI
- Scenario modeling and what-if analysis
- No need to manually calculate or use external tools
- Code execution happens securely in OpenAI's sandboxed environment
- Charts and graphs are automatically extracted and displayed inline in responses

## Project Structure

```
founder-copilot/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ openai_client.py     # OpenAI API wrapper
‚îÇ   ‚îú‚îÄ‚îÄ storage.py           # Local state management
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py           # Metrics tracking
‚îÇ   ‚îî‚îÄ‚îÄ static/
‚îÇ       ‚îú‚îÄ‚îÄ index.html        # Web UI
‚îÇ       ‚îî‚îÄ‚îÄ metrics.html      # Metrics dashboard
‚îú‚îÄ‚îÄ data/                     # Knowledge base files
‚îÇ   ‚îú‚îÄ‚îÄ yc_do_things_dont_scale.md
‚îÇ   ‚îú‚îÄ‚îÄ preseed_checklist.md
‚îÇ   ‚îî‚îÄ‚îÄ founder_scenario_b2b.md
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ seed_knowledge.py      # Initialize vector store and assistant
‚îÇ   ‚îî‚îÄ‚îÄ update_assistant.py    # Update existing assistant instructions
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Dockerfile
‚îî‚îÄ‚îÄ docker-compose.yml
```

## Setup

### Prerequisites

- Python 3.11+
- Docker and Docker Compose (optional)
- OpenAI API key

### 1. Clone and Configure

```bash
cd founder-copilot
cp .env.example .env
```

Edit `.env` and add your OpenAI API key:
```
OPENAI_API_KEY=your_api_key_here
OPENAI_MODEL=gpt-4.1  # Optional, defaults to gpt-4.1
COPILOT_NAME=FounderCopilot  # Optional, defaults to FounderCopilot
REDIS_URL=redis://localhost:6379/0  # Optional, defaults to redis://localhost:6379/0
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Seed the Knowledge Base

This creates the vector store, uploads knowledge base files, and creates the assistant:

```bash
python scripts/seed_knowledge.py
```

This will:
- Create a vector store named "founder_copilot_knowledge"
- Upload all `.md`, `.txt`, and `.json` files from the `data/` directory
- Create an assistant with access to the vector store and enforced file_search usage
- Save IDs to `.copilot_state.json`

**Note**: The assistant is configured to always use the file_search tool for every question, ensuring knowledge base retrieval.

**Updating Assistant Instructions**:
If you need to update the assistant's instructions without recreating it:
```bash
python scripts/update_assistant.py
```

### 4. Start Redis (Required for Rate Limiting)

**With Docker Compose** (recommended):
Redis is automatically started - no action needed.

**Local Development**:
```bash
# macOS
brew services start redis

# Linux
sudo systemctl start redis

# Or use Docker
docker run -d -p 6379:6379 redis:7-alpine
```

### 5. Run the Application

**Local Development**:
```bash
python -m uvicorn app.main:app --reload --port 8000
```

**Docker**:
```bash
docker compose up --build
```

The application will be available at `http://localhost:8000`

**Note**: If Redis is not available, the application will fail to start. Make sure Redis is running before starting the application.

## Usage

### Web Interface

1. Open `http://localhost:8000` in your browser
2. Start chatting with the assistant
3. Ask questions about startups, fundraising, B2B strategies, etc.

### API Endpoints

- `GET /` - Web UI
- `GET /health` - Health check (no rate limit)
- `POST /reset` - Create a new conversation thread (10 requests/minute per IP)
- `POST /chat` - Send a message to the assistant (3 requests/minute per IP)
  ```json
  {
    "message": "How do I raise pre-seed funding?"
  }
  ```
  
  **Request with File Upload** (multipart/form-data):
  ```bash
  curl -X POST http://localhost:8000/chat \
    -F "message=Visualize our Q1 progress vs targets" \
    -F "files=@kpi_tracker.csv"
  ```
  
  **Response** (structured format):
  ```json
  {
    "thread_id": "thread_abc123",
    "answer": "Pre-seed funding typically involves...",
    "bullets": [
      "Point 1: ...",
      "Point 2: ..."
    ],
    "sources": [
      {
        "file_id": "file_xyz789",
        "filename": "preseed_checklist.md",
        "quote": "Relevant quote from the source"
      }
    ],
    "images": [
      {
        "file_id": "file_chart123",
        "data_url": "data:image/png;base64,iVBORw0KGgo..."
      }
    ],
    "raw_text": "Full response text...",
    "usage": {
      "input_tokens": 150,
      "output_tokens": 200,
      "total_tokens": 350
    }
  }
  ```
  
  **Note**: The response includes structured data with:
  - `answer`: Main response text (citation markers cleaned)
  - `bullets`: Array of bullet points (if provided by assistant)
  - `sources`: Array of source citations with file IDs, filenames, and quotes
  - `images`: Array of chart/visualization images generated by code_interpreter (base64 data URLs)
  - `raw_text`: Original response text before processing
  - `usage`: Token usage statistics
- `GET /metrics` - Metrics dashboard (web UI, no rate limit)
- `GET /api/metrics` - Metrics data (JSON API, no rate limit)
- `POST /api/metrics/reset` - Reset all metrics (no rate limit)

**Note**: Rate limits are enforced per IP address. When rate limits are exceeded, the API returns a `429 Too Many Requests` status code.

### Adding Knowledge

To add more knowledge to the assistant:

1. Add markdown, text, or JSON files to the `data/` directory
2. Re-run the seed script:
   ```bash
   python scripts/seed_knowledge.py
   ```
   Or in Docker:
   ```bash
   docker compose exec foundercopilot python scripts/seed_knowledge.py
   ```

**Note**: Re-running the seed script creates a new vector store and assistant. If you want to keep the same assistant ID, you can manually add files to the existing vector store using the OpenAI API, or update the assistant instructions using `update_assistant.py`.

## Metrics & Monitoring

The application includes built-in metrics tracking to monitor performance and usage with time-based aggregations and visualizations.

### Metrics Dashboard

Access the metrics dashboard at `http://localhost:8000/metrics` to view:

- **Summary Cards** (14-day window)
  - Total requests
  - Error rate percentage
  - Average and P95 latency (in milliseconds)

- **Time-Series Charts**
  - **Requests per hour** (last 48 hours) - Bar chart showing request volume
  - **Tokens per hour** (last 48 hours) - Bar chart showing token consumption
  - **Average latency per hour** (last 48 hours) - Line chart showing latency trends
  - **Daily totals** (last 14 days) - Bar chart showing daily request counts

### Features

- **Event-Based Tracking** - Each request is tracked as an event with timestamp, latency, tokens, model, and error status
- **Rolling Window** - Maintains up to 5,000 events in memory (configurable)
- **Time-Based Aggregations** - Automatically groups data by hour and day for trend analysis
- **Visual Charts** - Interactive charts powered by Chart.js for easy visualization
- **Reset Functionality** - Clear all metrics with a single click
- **JSON API** - Access metrics programmatically via `/api/metrics`
- **Automatic Tracking** - All chat requests are automatically tracked

### API Usage

**Get Metrics (JSON)**:
```bash
curl http://localhost:8000/api/metrics
```

Response:
```json
{
  "totals": {
    "window_days": 14,
    "requests": 142,
    "errors": 2,
    "error_rate": 1.41,
    "latency_ms": {
      "avg": 1450.2,
      "p50": 1250.5,
      "p95": 2850.3
    },
    "tokens": {
      "input": 12500,
      "output": 8500,
      "total": 21000
    }
  },
  "hourly": [
    {
      "bucket_start_iso": "2024-01-15T10:00:00+00:00",
      "req": 5,
      "err": 0,
      "avg_latency_ms": 1200.5,
      "input_tokens": 500,
      "output_tokens": 300,
      "total_tokens": 800
    }
  ],
  "daily": [
    {
      "bucket_start_iso": "2024-01-15T00:00:00+00:00",
      "req": 42,
      "err": 1,
      "avg_latency_ms": 1350.2,
      "input_tokens": 4200,
      "output_tokens": 2800,
      "total_tokens": 7000
    }
  ]
}
```

**Response Structure**:
- `totals`: Aggregated statistics for the last 14 days
  - `window_days`: Time window in days
  - `requests`: Total number of requests
  - `errors`: Number of failed requests
  - `error_rate`: Error percentage
  - `latency_ms`: Object with `avg`, `p50`, `p95` latency values
  - `tokens`: Object with `input`, `output`, `total` token counts
- `hourly`: Array of hourly buckets for the last 48 hours
- `daily`: Array of daily buckets for the last 14 days

**Reset Metrics**:
```bash
curl -X POST http://localhost:8000/api/metrics/reset
```

### How Metrics Work

1. **Event Recording**: Every `/chat` request creates an event with:
   - Timestamp (milliseconds)
   - Latency (milliseconds)
   - Token usage (input, output, total)
   - Model name (if available)
   - Error status

2. **Rolling Window**: Events are stored in a deque with a maximum capacity (default: 5,000 events)

3. **Time-Based Aggregation**: Events are automatically grouped into:
   - **Hourly buckets**: Last 48 hours for detailed analysis
   - **Daily buckets**: Last 14 days for trend analysis

4. **In-Memory Storage**: Metrics are stored in memory (resets on server restart)

5. **Efficient Calculation**: Percentiles and aggregations are calculated on-demand from the event stream

**Note**: For multi-instance deployments, consider backing metrics with Redis or a database for persistence across instances.

## Rate Limiting

The application uses **FastAPI Limiter with Redis** to enforce rate limits on API endpoints. This protects the service from abuse and helps manage costs.

### Rate Limits

- **`/chat` endpoint**: 3 requests per 60 seconds per IP address
- **`/reset` endpoint**: 10 requests per 60 seconds per IP address
- **Other endpoints**: No rate limits applied

### How It Works

1. **Redis-Based**: Rate limiting state is stored in Redis, enabling distributed rate limiting across multiple instances
2. **IP-Based Identification**: Rate limits are enforced per IP address
3. **Proxy Support**: Automatically handles `X-Forwarded-For` and `CF-Connecting-IP` headers for accurate IP detection behind proxies
4. **Automatic Enforcement**: Rate limits are automatically enforced via FastAPI dependencies

### Rate Limit Responses

When a rate limit is exceeded, the API returns:

```json
{
  "detail": "Rate limit exceeded: 3 per 60 seconds"
}
```

With HTTP status code `429 Too Many Requests`.

### Configuration

Rate limiting requires Redis to be running. The application connects to Redis using the `REDIS_URL` environment variable:

- **Default**: `redis://localhost:6379/0`
- **Docker**: Automatically configured via `docker-compose.yml`

### Redis Setup

**With Docker Compose** (recommended):
Redis is automatically started when you run `docker compose up`. No additional configuration needed.

**Local Development**:
```bash
# Install and start Redis
# macOS
brew install redis
brew services start redis

# Linux
sudo apt-get install redis-server
sudo systemctl start redis

# Or use Docker
docker run -d -p 6379:6379 redis:7-alpine
```

### Customizing Rate Limits

To modify rate limits, edit the `RateLimiter` decorators in `app/main.py`:

```python
# Current: 3 requests per 60 seconds
@app.post("/chat", dependencies=[Depends(RateLimiter(times=3, seconds=60))])

# Example: 10 requests per minute
@app.post("/chat", dependencies=[Depends(RateLimiter(times=10, seconds=60))])

# Example: 100 requests per hour
@app.post("/chat", dependencies=[Depends(RateLimiter(times=100, seconds=3600))])
```

## Configuration

### Environment Variables

- `OPENAI_API_KEY` (required) - Your OpenAI API key
- `OPENAI_MODEL` (optional) - Model to use, defaults to `gpt-4.1`
- `COPILOT_NAME` (optional) - Assistant name, defaults to `FounderCopilot`
- `APP_PORT` (optional) - Server port, defaults to `8000`
- `REDIS_URL` (optional) - Redis connection URL, defaults to `redis://localhost:6379/0`

### State Management

The application stores assistant and vector store IDs in a state file:
- **Local development**: `.copilot_state.json` in the project root
- **Docker**: `state/copilot_state.json` (persisted via Docker volume)

This file is:
- Created automatically when you run `seed_knowledge.py`
- Used by the application to connect to the correct assistant
- Should be committed to version control (or ignored if you prefer)
- In Docker, the `state/` directory is mounted as a volume for persistence

## Docker Commands

```bash
# Build and start (includes Redis)
docker compose up --build

# Run seed script in container
docker compose exec foundercopilot python scripts/seed_knowledge.py

# Update assistant instructions (keeps same assistant ID)
docker compose exec foundercopilot python scripts/update_assistant.py

# View logs (all services)
docker compose logs -f

# View logs (specific service)
docker compose logs -f foundercopilot
docker compose logs -f redis

# Stop all services
docker compose down

# Stop and remove volumes
docker compose down -v
```

**Note**: The `docker-compose.yml` includes both the application and Redis services. When you run `docker compose up`, both services start automatically.

## How It Works

1. **Knowledge Base Setup**: Files in `data/` are uploaded to a vector store, where they're chunked and indexed for semantic search.

2. **Assistant Creation**: An assistant is created with:
   - Access to the vector store via file search tool
   - Code interpreter tool for analytics and calculations
   - Custom instructions for YC-style startup advice
   - Ability to retrieve and cite relevant information
   - Ability to run Python code for financial analysis and visualizations

3. **Conversation Flow**:
   - User sends a message via the web UI or API (optionally with file attachments)
   - If files are attached, they are uploaded to OpenAI and attached to the message
   - Message is added to a conversation thread
   - Assistant runs on the thread, automatically using file search for every question
   - For analytics questions, the assistant uses code_interpreter to run Python code
   - Relevant knowledge base snippets are retrieved and included in the response
   - Calculations, charts, and visualizations are generated when needed
   - Charts and images generated by code_interpreter are automatically extracted
   - Images are downloaded from OpenAI and converted to base64 data URLs
   - Source citations are extracted from message annotations
   - Citation markers are cleaned from the response text
   - Structured response (answer, bullets, sources, images) is returned to the user
   - UI displays the response with inline charts and visualizations

4. **Retrieval Process**:
   - When the assistant needs information, it uses the file search tool
   - The tool searches the vector store for semantically similar content
   - Top relevant chunks are retrieved and provided as context
   - The assistant synthesizes the information into a helpful response
   - File citations are automatically added as annotations in the message

5. **Source Extraction**:
   - Source citations are extracted from message annotations (primary method)
   - Fallback extraction from run steps if annotations aren't available
   - File IDs are enriched with filenames by querying the OpenAI Files API
   - Citation markers (e.g., `„Äê4:0‚Ä†filename.md„Äë`) are automatically cleaned from text
   - Sources are displayed in the UI with filenames and quotes

6. **Image/Chart Extraction**:
   - When code_interpreter generates charts or visualizations, they are returned as `image_file` content parts
   - Image file IDs are extracted from message content
   - Images are downloaded from OpenAI using the Files API
   - Images are converted to base64 data URLs for direct browser display
   - Content type (PNG, JPEG, etc.) is automatically detected from file metadata
   - Images are included in the structured response and displayed inline in the UI

## Requirements

- `openai==1.51.2` - OpenAI Python SDK
- `fastapi==0.115.5` - Web framework
- `uvicorn[standard]==0.32.0` - ASGI server
- `python-dotenv==1.0.1` - Environment variable management
- `redis==5.0.6` - Redis client for rate limiting
- `fastapi-limiter==0.1.6` - Rate limiting for FastAPI
- `python-multipart==0.0.9` - Form data parsing for file uploads
- `httpx==0.27.2` - HTTP client (dependency)
- `jinja2==3.1.4` - Template engine (dependency)

## License

MIT
