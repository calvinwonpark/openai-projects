# Founder Copilot

An AI-powered assistant for startup founders, built with OpenAI's Assistant API. Get personalized advice on fundraising, B2B strategies, and startup best practices using a knowledge base of curated startup resources.

## Features

- ü§ñ **AI Assistant** - Powered by GPT-4 with retrieval-augmented generation (RAG)
- üéØ **Multi-Assistant Routing** - Intelligent routing to specialized assistants (Tech, Marketing, Investor)
- üìö **Knowledge Base** - Vector store containing startup resources (YC advice, checklists, scenarios)
- üí¨ **Modern Chat Interface** - Beautiful, responsive web UI with structured responses
- ‚ö° **Real-Time Streaming** - Server-Sent Events (SSE) for live response streaming with visual indicators
- üîç **File Search** - Automatic retrieval of relevant information from knowledge base
- üìù **Source Citations** - Automatic extraction and display of source files with quotes
- üßÆ **Code Interpreter** - Python code execution for founder analytics, calculations, and visualizations
- üìà **Chart Display** - Automatic extraction and display of charts/visualizations generated by code_interpreter
- üìé **File Upload** - Upload CSV, Excel, or other files for analysis and visualization
- üìã **Product Card System** - Automatic extraction and management of product context for deictic references
- üîÑ **File Persistence** - Automatic re-attachment of files across follow-up questions in data analysis flows
- üìä **Metrics Dashboard** - Track token usage, latency (P95), and request statistics
- üö¶ **Rate Limiting** - Redis-based rate limiting to protect API endpoints
- üê≥ **Docker Support** - Easy deployment with Docker Compose

## Project Evolution

This project has evolved from a simple MVP to a sophisticated multi-assistant system. Here's the progression:

### Phase 1: MVP (Minimum Viable Product)
- **Basic Assistant**: Single GPT-4 assistant with OpenAI Assistants API
- **Knowledge Base**: Vector store with startup resources (YC advice, checklists)
- **Simple Chat Interface**: Basic web UI for interacting with the assistant
- **File Search**: Automatic retrieval from knowledge base using file_search tool
- **Core Functionality**: Ask questions, get answers based on knowledge base

### Phase 2: Production Readiness
- **Rate Limiting**: Redis-based rate limiting to protect API endpoints (3 req/60s for chat, 10 req/60s for reset)
- **Docker Support**: Containerized deployment with Docker Compose
- **State Management**: Persistent storage of assistant and vector store IDs
- **Error Handling**: Robust error handling and user feedback

### Phase 3: Enhanced User Experience
- **Structured Output**: Parsed JSON responses with answer, bullets, and structured data
- **Source Citations**: Automatic extraction and display of source files with quotes
- **Citation Cleaning**: Removal of citation markers (e.g., `„Äê4:0‚Ä†filename.md„Äë`) from response text
- **Modern UI**: Redesigned chat interface with chat bubbles, loading states, and example questions
- **Source Display**: Dedicated "Sources" section showing filenames and quoted snippets

### Phase 4: Analytics & Monitoring
- **Metrics Dashboard**: Real-time tracking of token usage, latency (P95), and request statistics
- **Time-Series Aggregations**: Hourly (48h) and daily (14d) metrics with Chart.js visualizations
- **Event-Based Tracking**: Each request tracked as an event with comprehensive metadata
- **Metrics API**: JSON API for programmatic access to metrics data

### Phase 5: Advanced Capabilities
- **Code Interpreter**: Python code execution for financial calculations, data analysis, and visualizations
- **Chart Display**: Automatic extraction and display of charts/visualizations generated by code_interpreter
- **File Upload**: Upload CSV, Excel, or other files for analysis and visualization
- **Image Extraction**: Download and convert images from code_interpreter to base64 data URLs
- **Financial Analytics**: Calculate burn rate, runway, KPIs, and create visualizations

### Phase 6: Multi-Assistant Routing
- **Specialized Assistants**: Three domain-specific assistants (TechAdvisor, MarketingAdvisor, InvestorAdvisor)
- **Intelligent Routing**: Hybrid heuristic + classifier approach for query routing
- **Data Isolation**: Separate vector stores per assistant to maintain knowledge base separation
- **Scope Enforcement**: Assistants defer to others for out-of-scope questions
- **Routing Strategies**: 
  - Winner-take-all (high confidence): Single assistant responds
  - Consult-then-decide (moderate confidence or high-risk): Primary answers, reviewer provides Devil's Advocate critique
  - Parallel ensemble (low confidence or ambiguous queries): Both assistants answer independently, perspectives presented equally
- **Tool Scoping**: code_interpreter enabled only where needed (Tech and Investor advisors)
- **Ensemble Responses**: Composed responses from multiple assistants when appropriate

### Phase 7: Real-Time Streaming
- **Streaming API**: Server-Sent Events (SSE) for live response streaming
- **Real-Time Updates**: Text streams in character-by-character as the assistant generates it
- **Streaming Indicator**: Animated dots (". . .") show when stream is active vs complete
- **JSON Extraction**: Automatic extraction of answer from JSON responses during streaming
- **Phase-Aware Streaming**: Separate tracking of primary and reviewer responses in multi-assistant mode
- **Live Formatting**: Markdown formatting applied in real-time as text streams
- **Progressive Display**: Primary response streams immediately, reviewer comments append when ready

### Phase 8: Context-Aware Product Cards & File Persistence (Current)
- **Product Card Auto-Extraction**: Automatically detects and extracts product information from user messages
- **Product Card Management**: Lightweight, neutral product cards (‚â§ 200-300 tokens) with versioning
- **Deictic Reference Detection**: Identifies context-dependent language ("this product", "my app", "above")
- **Message Rewriting**: Automatically prepends product card context to messages with deictic references
- **Shared Analysis Thread**: Dedicated thread for data analysis flows (CSV/visualization iterations)
- **File Persistence**: Automatic re-attachment of files across follow-up questions in data flows
- **Context Injection**: Product cards and produced files automatically attached to relevant messages
- **Clarification Prompts**: Asks user to select product when multiple products exist and confidence is low

### Key Milestones
1. ‚úÖ **MVP Launch**: Basic assistant with knowledge base
2. ‚úÖ **Rate Limiting**: Production-ready API protection
3. ‚úÖ **Structured Output**: Enhanced response format with citations
4. ‚úÖ **Metrics Dashboard**: Comprehensive monitoring and analytics
5. ‚úÖ **Code Interpreter**: Financial analytics and visualizations
6. ‚úÖ **File Upload**: CSV/Excel analysis capabilities
7. ‚úÖ **Multi-Assistant System**: Intelligent routing to specialized assistants
8. ‚úÖ **Real-Time Streaming**: Live response streaming with visual indicators
9. ‚úÖ **Product Cards & File Persistence**: Context-aware routing with automatic file management

## OpenAI API Usage

This project uses **OpenAI's Assistants API (Beta)** to create an intelligent assistant with retrieval capabilities. Here's how each API component is used:

### 1. Vector Stores API (Beta)

**Purpose**: Store and manage knowledge base files for retrieval

**Usage**:
- **`vector_stores.create()`** - Creates a new vector store to hold knowledge base files
- **`vector_stores.file_batches.upload_and_poll()`** - Uploads multiple files in a batch and polls for completion
  - Supports `.md`, `.txt`, and `.json` files
  - Automatically chunks and indexes files for semantic search

**Implementation**:
```python
# Create vector store
vs = client.beta.vector_stores.create(name="founder_copilot_knowledge")

# Upload files in batch
batch = client.beta.vector_stores.file_batches.upload_and_poll(
    vector_store_id=vs.id,
    files=[open("data/file1.md", "rb"), open("data/file2.md", "rb")]
)
```

### 2. Assistants API (Beta)

**Purpose**: Create and manage AI assistants with tools and knowledge

**Usage**:
- **`assistants.create()`** - Creates an assistant with:
  - Custom instructions (YC-style startup advisor persona)
  - Model selection (configurable via `OPENAI_MODEL`, defaults to `gpt-4.1`)
  - File search tool for retrieval from vector stores
  - Code interpreter tool for analytics and calculations
  - Vector store integration for knowledge base access

**Implementation**:
```python
assistant = client.beta.assistants.create(
    name="FounderCopilot",
    model="gpt-4.1",
    instructions="You are a YC-style startup advisor...",
    tools=[
        {"type": "file_search"},
        {"type": "code_interpreter"}  # For founder analytics
    ],
    tool_resources={
        "file_search": {
            "vector_store_ids": [vector_store_id]
        }
    }
)
```

### 3. Threads API (Beta)

**Purpose**: Manage conversation threads for multi-turn dialogues

**Usage**:
- **`threads.create()`** - Creates a new conversation thread
- **`threads.messages.create()`** - Adds messages to a thread (user or assistant)
- **`threads.runs.create()`** - Executes the assistant on a thread
- **`threads.runs.retrieve()`** - Polls run status until completion
- **`threads.messages.list()`** - Retrieves conversation history

**Implementation**:
```python
# Create thread
thread = client.beta.threads.create()

# Add user message
client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="How do I raise pre-seed funding?"
)

# Run assistant
run = client.beta.threads.runs.create(
    thread_id=thread.id,
    assistant_id=assistant.id
)

# Poll for completion
while run.status not in ("completed", "failed"):
    run = client.beta.threads.runs.retrieve(
        thread_id=thread.id,
        run_id=run.id
    )
```

### 4. File Search Tool

**Purpose**: Enable the assistant to retrieve relevant information from the vector store

**How it works**:
- When a user asks a question, the assistant automatically uses the `file_search` tool
- The tool searches the vector store for semantically similar content
- Relevant snippets are retrieved and included in the assistant's response
- The assistant cites sources when referencing knowledge base content
- Source citations are automatically extracted from message annotations

**Benefits**:
- Up-to-date information without retraining models
- Ability to add/update knowledge base files without changing the assistant
- Automatic relevance ranking and retrieval
- Citation of sources for transparency
- Structured output with answer, bullets, and source references

### 5. Code Interpreter Tool

**Purpose**: Enable the assistant to run Python code for founder analytics, calculations, and data visualization

**How it works**:
- The assistant automatically uses `code_interpreter` when questions involve:
  - Financial calculations (burn rate, runway, growth rates, etc.)
  - Data analysis and statistical computations
  - Creating charts and visualizations
  - Modeling scenarios and what-if analyses
  - Performing complex calculations that require code execution

**Use Cases**:
- **Financial Metrics**: Calculate burn rate, runway, customer acquisition cost (CAC), lifetime value (LTV), etc.
- **Growth Analysis**: Analyze growth rates, cohort analysis, conversion funnels
- **Projections**: Model revenue projections, funding scenarios, hiring plans
- **Visualizations**: Create charts and graphs for metrics, trends, and comparisons
- **Statistical Analysis**: Perform regression analysis, A/B test calculations, correlation analysis
- **File Analysis**: Upload CSV/Excel files and analyze data, create visualizations, and generate insights

**Example Questions**:
- "If I have $500K in the bank and burn $50K/month, how long is my runway?"
- "Calculate my CAC:LTV ratio if CAC is $100 and LTV is $500"
- "Create a chart showing revenue growth over the last 6 months"
- "What's my break-even point if fixed costs are $20K/month and margin is 60%?"
- "Here's kpi_tracker.csv ‚Äî visualize our Q1 progress vs targets"

**File Upload Support**:
- Upload CSV, Excel, JSON, or text files via the web UI
- Files are automatically attached to your message
- The assistant can read, analyze, and visualize data from uploaded files
- Charts and visualizations generated from file data are automatically displayed in the chat

**Benefits**:
- Accurate calculations for financial and business metrics
- Dynamic visualizations and charts that are automatically displayed in the UI
- Scenario modeling and what-if analysis
- No need to manually calculate or use external tools
- Code execution happens securely in OpenAI's sandboxed environment
- Charts and graphs are automatically extracted and displayed inline in responses

## Project Structure

```
founder-copilot/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI application with routing
‚îÇ   ‚îú‚îÄ‚îÄ openai_client.py     # OpenAI API wrapper
‚îÇ   ‚îú‚îÄ‚îÄ storage.py           # Local state management
‚îÇ   ‚îú‚îÄ‚îÄ router.py            # Multi-assistant routing logic
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py           # Metrics tracking
‚îÇ   ‚îî‚îÄ‚îÄ static/
‚îÇ       ‚îú‚îÄ‚îÄ index.html        # Web UI
‚îÇ       ‚îî‚îÄ‚îÄ metrics.html      # Metrics dashboard
‚îú‚îÄ‚îÄ data/                     # Knowledge base files
‚îÇ   ‚îú‚îÄ‚îÄ tech/                 # Technical documentation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ architecture_patterns.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scalability_principles.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ai_ml_deployment.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api_design.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ security_best_practices.md
‚îÇ   ‚îú‚îÄ‚îÄ marketing/            # Marketing resources
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ growth_tactics.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ copywriting_tips.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ customer_acquisition.md
‚îÇ   ‚îú‚îÄ‚îÄ investor/             # Fundraising resources
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fundraising_strategy.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pitch_deck_structure.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ kpi_definitions.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ kpi_tracker.csv
‚îÇ   ‚îú‚îÄ‚îÄ yc_do_things_dont_scale.md
‚îÇ   ‚îú‚îÄ‚îÄ preseed_checklist.md
‚îÇ   ‚îî‚îÄ‚îÄ founder_scenario_b2b.md
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ seed_knowledge.py         # Initialize single assistant (legacy)
‚îÇ   ‚îú‚îÄ‚îÄ seed_multi_assistants.py  # Initialize multi-assistant system
‚îÇ   ‚îî‚îÄ‚îÄ update_assistant.py      # Update existing assistant instructions
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ MULTI_ASSISTANT_ROUTING.md   # Multi-assistant routing documentation
‚îî‚îÄ‚îÄ EXAMPLE_QUESTIONS.md         # Example questions for testing
```

## Setup

### Prerequisites

- Python 3.11+
- Docker and Docker Compose (optional)
- OpenAI API key

### 1. Clone and Configure

```bash
cd founder-copilot
cp .env.example .env
```

Edit `.env` and add your OpenAI API key:
```
OPENAI_API_KEY=your_api_key_here
OPENAI_MODEL=gpt-4.1  # Optional, defaults to gpt-4.1
COPILOT_NAME=FounderCopilot  # Optional, defaults to FounderCopilot
REDIS_URL=redis://localhost:6379/0  # Optional, defaults to redis://localhost:6379/0
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Seed the Knowledge Base

You have two options:

#### Option A: Multi-Assistant System (Recommended)

This creates three specialized assistants with separate vector stores:

```bash
python scripts/seed_multi_assistants.py
```

This will:
- Create three vector stores (one per assistant):
  - `tech_knowledge` - For TechAdvisor
  - `marketing_knowledge` - For MarketingAdvisor
  - `investor_knowledge` - For InvestorAdvisor
- Upload files from `data/tech/`, `data/marketing/`, and `data/investor/` directories
- Create three specialized assistants:
  - **TechAdvisor** - Architecture, AI/ML patterns, system design (code_interpreter enabled)
  - **MarketingAdvisor** - Launch/growth/copywriting (file_search only)
  - **InvestorAdvisor** - Fundraising, KPIs, pitch decks (code_interpreter enabled)
- Save all assistant IDs to `.copilot_state.json`

**Benefits of Multi-Assistant System**:
- Intelligent routing to the right specialist
- Data isolation (each assistant has its own knowledge base)
- Scope enforcement (assistants defer to others for out-of-scope questions)
- Consult-then-decide for ambiguous or high-risk queries (primary answers, reviewer provides Devil's Advocate critique)
- Parallel ensemble for low-confidence queries

#### Option B: Single Assistant (Legacy)

This creates a single general-purpose assistant:

```bash
python scripts/seed_knowledge.py
```

This will:
- Create a vector store named "founder_copilot_knowledge"
- Upload all `.md`, `.txt`, and `.json` files from the `data/` directory
- Create a single assistant with access to the vector store
- Save IDs to `.copilot_state.json`

**Note**: The system automatically detects which setup you have and uses the appropriate mode. Multi-assistant setup takes precedence if available.

**Updating Assistant Instructions**:
If you need to update the assistant's instructions without recreating it:
```bash
python scripts/update_assistant.py
```

### 4. Start Redis (Required for Rate Limiting)

**With Docker Compose** (recommended):
Redis is automatically started - no action needed.

**Local Development**:
```bash
# macOS
brew services start redis

# Linux
sudo systemctl start redis

# Or use Docker
docker run -d -p 6379:6379 redis:7-alpine
```

### 5. Run the Application

**Local Development**:
```bash
python -m uvicorn app.main:app --reload --port 8000
```

**Docker**:
```bash
docker compose up --build
```

The application will be available at `http://localhost:8000`

**Note**: If Redis is not available, the application will fail to start. Make sure Redis is running before starting the application.

## Usage

### Web Interface

1. Open `http://localhost:8000` in your browser
2. Start chatting with the assistant
3. Ask questions about startups, fundraising, B2B strategies, etc.

### API Endpoints

- `GET /` - Web UI
- `GET /health` - Health check (no rate limit)
- `POST /reset` - Create a new conversation thread (10 requests/minute per IP)
- `POST /chat` - Send a message to the assistant (3 requests/minute per IP) - **Legacy non-streaming endpoint**
- `POST /chat/stream` - Send a message with real-time streaming (3 requests/minute per IP) - **Recommended**
  ```json
  {
    "message": "How do I raise pre-seed funding?"
  }
  ```
  
  **Request with File Upload** (multipart/form-data):
  ```bash
  curl -X POST http://localhost:8000/chat \
    -F "message=Visualize our Q1 progress vs targets" \
    -F "files=@kpi_tracker.csv"
  ```
  
  **Response** (structured format):
  ```json
  {
    "thread_id": "thread_abc123",
    "answer": "Pre-seed funding typically involves...",
    "bullets": [
      "Point 1: ...",
      "Point 2: ..."
    ],
    "sources": [
      {
        "file_id": "file_xyz789",
        "filename": "preseed_checklist.md",
        "quote": "Relevant quote from the source"
      }
    ],
    "images": [
      {
        "file_id": "file_chart123",
        "data_url": "data:image/png;base64,iVBORw0KGgo..."
      }
    ],
    "raw_text": "Full response text...",
    "usage": {
      "input_tokens": 150,
      "output_tokens": 200,
      "total_tokens": 350
    }
  }
  ```
  
  **Note**: The response includes structured data with:
  - `answer`: Main response text (citation markers cleaned)
  - `bullets`: Array of bullet points (if provided by assistant)
  - `sources`: Array of source citations with file IDs, filenames, and quotes
  - `images`: Array of chart/visualization images generated by code_interpreter (base64 data URLs)
  - `raw_text`: Original response text before processing
  - `usage`: Token usage statistics
  - `routing`: (Multi-assistant mode only) Routing information including label, confidence, top2_label, margin, and is_high_risk

- `POST /chat/stream` - **Streaming chat endpoint** (Server-Sent Events)
  
  **Request** (multipart/form-data):
  ```bash
  curl -X POST http://localhost:8000/chat/stream \
    -F "message=What tools should I use for deployment?" \
    -F "files=@kpi_tracker.csv"
  ```
  
  **Response** (Server-Sent Events stream):
  ```
  data: {"type":"routing","strategy":"consult_then_decide","primary_label":"tech","reviewer_label":"marketing","confidence":0.65}
  
  data: {"type":"text_delta","content":"For","accumulated":"For"}
  
  data: {"type":"text_delta","content":" zero","accumulated":"For zero"}
  
  data: {"type":"text_delta","content":"-downtime","accumulated":"For zero-downtime"}
  
  data: {"type":"sources","sources":[{"file_id":"file_xyz","filename":"deployment.md","quote":"..."}]}
  
  data: {"type":"phase_transition","from":"primary","to":"reviewer"}
  
  data: {"type":"text_delta","content":"Here","phase":"reviewer","accumulated":"Here"}
  
  data: {"type":"done","answer":"...","bullets":[...],"sources":[...],"images":[...],"usage":{...},"phase":"reviewer"}
  ```
  
  **Stream Event Types**:
  - `routing`: Routing decision (strategy, labels, confidence)
  - `text_delta`: Incremental text updates (`content` = new chunk, `accumulated` = full text so far)
  - `sources`: Source citations update
  - `images`: Image/chart updates
  - `phase_transition`: Transition between primary and reviewer phases (multi-assistant mode)
  - `done`: Stream complete with final structured data
  - `error`: Error occurred during streaming
  
  **Features**:
  - **Real-time streaming**: Text appears as it's generated
  - **Streaming indicator**: Animated dots (". . .") show active streaming
  - **JSON extraction**: Automatically extracts answer from JSON responses during streaming
  - **Phase-aware**: Tracks primary and reviewer responses separately in multi-assistant mode
  - **Progressive display**: Primary response streams first, reviewer comments append when ready
  - **Live formatting**: Markdown formatting applied in real-time
  - **Citation cleaning**: Citation markers removed during streaming
- `GET /metrics` - Metrics dashboard (web UI, no rate limit)
- `GET /api/metrics` - Metrics data (JSON API, no rate limit)
- `POST /api/metrics/reset` - Reset all metrics (no rate limit)

**Note**: Rate limits are enforced per IP address. When rate limits are exceeded, the API returns a `429 Too Many Requests` status code.

### Adding Knowledge

#### For Multi-Assistant System

1. Add files to the appropriate directory:
   - Technical docs ‚Üí `data/tech/`
   - Marketing resources ‚Üí `data/marketing/`
   - Fundraising/financial ‚Üí `data/investor/`
2. Re-run the seed script:
   ```bash
   python scripts/seed_multi_assistants.py
   ```
   Or in Docker:
   ```bash
   docker compose exec foundercopilot python scripts/seed_multi_assistants.py
   ```

#### For Single Assistant (Legacy)

1. Add markdown, text, or JSON files to the `data/` directory
2. Re-run the seed script:
   ```bash
   python scripts/seed_knowledge.py
   ```
   Or in Docker:
   ```bash
   docker compose exec foundercopilot python scripts/seed_knowledge.py
   ```

**Note**: Re-running the seed script creates new vector stores and assistants. If you want to keep the same assistant IDs, you can manually add files to existing vector stores using the OpenAI API, or update assistant instructions using `update_assistant.py`.

## Metrics & Monitoring

The application includes built-in metrics tracking to monitor performance and usage with time-based aggregations and visualizations.

### Metrics Dashboard

Access the metrics dashboard at `http://localhost:8000/metrics` to view:

- **Summary Cards** (14-day window)
  - Total requests
  - Error rate percentage
  - Average and P95 latency (in milliseconds)

- **Time-Series Charts**
  - **Requests per hour** (last 48 hours) - Bar chart showing request volume
  - **Tokens per hour** (last 48 hours) - Bar chart showing token consumption
  - **Average latency per hour** (last 48 hours) - Line chart showing latency trends
  - **Daily totals** (last 14 days) - Bar chart showing daily request counts

### Features

- **Event-Based Tracking** - Each request is tracked as an event with timestamp, latency, tokens, model, and error status
- **Rolling Window** - Maintains up to 5,000 events in memory (configurable)
- **Time-Based Aggregations** - Automatically groups data by hour and day for trend analysis
- **Visual Charts** - Interactive charts powered by Chart.js for easy visualization
- **Reset Functionality** - Clear all metrics with a single click
- **JSON API** - Access metrics programmatically via `/api/metrics`
- **Automatic Tracking** - All chat requests are automatically tracked

### API Usage

**Get Metrics (JSON)**:
```bash
curl http://localhost:8000/api/metrics
```

Response:
```json
{
  "totals": {
    "window_days": 14,
    "requests": 142,
    "errors": 2,
    "error_rate": 1.41,
    "latency_ms": {
      "avg": 1450.2,
      "p50": 1250.5,
      "p95": 2850.3
    },
    "tokens": {
      "input": 12500,
      "output": 8500,
      "total": 21000
    }
  },
  "hourly": [
    {
      "bucket_start_iso": "2024-01-15T10:00:00+00:00",
      "req": 5,
      "err": 0,
      "avg_latency_ms": 1200.5,
      "input_tokens": 500,
      "output_tokens": 300,
      "total_tokens": 800
    }
  ],
  "daily": [
    {
      "bucket_start_iso": "2024-01-15T00:00:00+00:00",
      "req": 42,
      "err": 1,
      "avg_latency_ms": 1350.2,
      "input_tokens": 4200,
      "output_tokens": 2800,
      "total_tokens": 7000
    }
  ]
}
```

**Response Structure**:
- `totals`: Aggregated statistics for the last 14 days
  - `window_days`: Time window in days
  - `requests`: Total number of requests
  - `errors`: Number of failed requests
  - `error_rate`: Error percentage
  - `latency_ms`: Object with `avg`, `p50`, `p95` latency values
  - `tokens`: Object with `input`, `output`, `total` token counts
- `hourly`: Array of hourly buckets for the last 48 hours
- `daily`: Array of daily buckets for the last 14 days

**Reset Metrics**:
```bash
curl -X POST http://localhost:8000/api/metrics/reset
```

### How Metrics Work

1. **Event Recording**: Every `/chat` request creates an event with:
   - Timestamp (milliseconds)
   - Latency (milliseconds)
   - Token usage (input, output, total)
   - Model name (if available)
   - Error status

2. **Rolling Window**: Events are stored in a deque with a maximum capacity (default: 5,000 events)

3. **Time-Based Aggregation**: Events are automatically grouped into:
   - **Hourly buckets**: Last 48 hours for detailed analysis
   - **Daily buckets**: Last 14 days for trend analysis

4. **In-Memory Storage**: Metrics are stored in memory (resets on server restart)

5. **Efficient Calculation**: Percentiles and aggregations are calculated on-demand from the event stream

**Note**: For multi-instance deployments, consider backing metrics with Redis or a database for persistence across instances.

## Rate Limiting

The application uses **FastAPI Limiter with Redis** to enforce rate limits on API endpoints. This protects the service from abuse and helps manage costs.

### Rate Limits

- **`/chat` endpoint**: 3 requests per 60 seconds per IP address
- **`/reset` endpoint**: 10 requests per 60 seconds per IP address
- **Other endpoints**: No rate limits applied

### How It Works

1. **Redis-Based**: Rate limiting state is stored in Redis, enabling distributed rate limiting across multiple instances
2. **IP-Based Identification**: Rate limits are enforced per IP address
3. **Proxy Support**: Automatically handles `X-Forwarded-For` and `CF-Connecting-IP` headers for accurate IP detection behind proxies
4. **Automatic Enforcement**: Rate limits are automatically enforced via FastAPI dependencies

### Rate Limit Responses

When a rate limit is exceeded, the API returns:

```json
{
  "detail": "Rate limit exceeded: 3 per 60 seconds"
}
```

With HTTP status code `429 Too Many Requests`.

### Configuration

Rate limiting requires Redis to be running. The application connects to Redis using the `REDIS_URL` environment variable:

- **Default**: `redis://localhost:6379/0`
- **Docker**: Automatically configured via `docker-compose.yml`

### Redis Setup

**With Docker Compose** (recommended):
Redis is automatically started when you run `docker compose up`. No additional configuration needed.

**Local Development**:
```bash
# Install and start Redis
# macOS
brew install redis
brew services start redis

# Linux
sudo apt-get install redis-server
sudo systemctl start redis

# Or use Docker
docker run -d -p 6379:6379 redis:7-alpine
```

### Customizing Rate Limits

To modify rate limits, edit the `RateLimiter` decorators in `app/main.py`:

```python
# Current: 3 requests per 60 seconds
@app.post("/chat", dependencies=[Depends(RateLimiter(times=3, seconds=60))])

# Example: 10 requests per minute
@app.post("/chat", dependencies=[Depends(RateLimiter(times=10, seconds=60))])

# Example: 100 requests per hour
@app.post("/chat", dependencies=[Depends(RateLimiter(times=100, seconds=3600))])
```

## Configuration

### Environment Variables

- `OPENAI_API_KEY` (required) - Your OpenAI API key
- `OPENAI_MODEL` (optional) - Model to use, defaults to `gpt-4.1`
- `COPILOT_NAME` (optional) - Assistant name, defaults to `FounderCopilot`
- `APP_PORT` (optional) - Server port, defaults to `8000`
- `REDIS_URL` (optional) - Redis connection URL, defaults to `redis://localhost:6379/0`

### State Management

The application stores assistant and vector store IDs in a state file:
- **Local development**: `.copilot_state.json` in the project root
- **Docker**: `state/copilot_state.json` (persisted via Docker volume)

This file is:
- Created automatically when you run `seed_knowledge.py` or `seed_multi_assistants.py`
- Used by the application to connect to the correct assistant(s)
- For multi-assistant mode: Contains IDs for all three assistants
- For single-assistant mode: Contains single assistant and vector store IDs
- Should be committed to version control (or ignored if you prefer)
- In Docker, the `state/` directory is mounted as a volume for persistence

## Docker Commands

```bash
# Build and start (includes Redis)
docker compose up --build

# Run seed script in container (single assistant)
docker compose exec foundercopilot python scripts/seed_knowledge.py

# Run seed script in container (multi-assistant)
docker compose exec foundercopilot python scripts/seed_multi_assistants.py

# Update assistant instructions (keeps same assistant ID)
docker compose exec foundercopilot python scripts/update_assistant.py

# View logs (all services)
docker compose logs -f

# View logs (specific service)
docker compose logs -f foundercopilot
docker compose logs -f redis

# Stop all services
docker compose down

# Stop and remove volumes
docker compose down -v
```

**Note**: The `docker-compose.yml` includes both the application and Redis services. When you run `docker compose up`, both services start automatically.

## How It Works

### Single Assistant Mode (Legacy)

1. **Knowledge Base Setup**: Files in `data/` are uploaded to a vector store, where they're chunked and indexed for semantic search.

2. **Assistant Creation**: An assistant is created with:
   - Access to the vector store via file search tool
   - Code interpreter tool for analytics and calculations
   - Custom instructions for YC-style startup advice
   - Ability to retrieve and cite relevant information
   - Ability to run Python code for financial analysis and visualizations

3. **Conversation Flow**:
   - User sends a message via the web UI or API (optionally with file attachments)
   - If files are attached, they are uploaded to OpenAI and attached to the message
   - Message is added to a conversation thread
   - **Streaming mode** (recommended): Assistant response streams in real-time via Server-Sent Events
     - Text appears character-by-character as it's generated
     - Streaming indicator (animated dots) shows active streaming
     - JSON responses are automatically parsed and extracted during streaming
     - Markdown formatting applied in real-time
     - Citation markers cleaned during streaming
   - **Non-streaming mode** (legacy): Assistant runs to completion, then returns full response
   - Assistant automatically uses file search for every question
   - For analytics questions, the assistant uses code_interpreter to run Python code
   - Relevant knowledge base snippets are retrieved and included in the response
   - Calculations, charts, and visualizations are generated when needed
   - Charts and images generated by code_interpreter are automatically extracted
   - Images are downloaded from OpenAI and converted to base64 data URLs
   - Source citations are extracted from message annotations
   - Citation markers are cleaned from the response text
   - Structured response (answer, bullets, sources, images) is returned to the user
   - UI displays the response with inline charts and visualizations

4. **Retrieval Process**:
   - When the assistant needs information, it uses the file search tool
   - The tool searches the vector store for semantically similar content
   - Top relevant chunks are retrieved and provided as context
   - The assistant synthesizes the information into a helpful response
   - File citations are automatically added as annotations in the message

5. **Source Extraction**:
   - Source citations are extracted from message annotations (primary method)
   - Fallback extraction from run steps if annotations aren't available
   - File IDs are enriched with filenames by querying the OpenAI Files API
   - Citation markers (e.g., `„Äê4:0‚Ä†filename.md„Äë`) are automatically cleaned from text
   - Sources are displayed in the UI with filenames and quotes

6. **Image/Chart Extraction**:
   - When code_interpreter generates charts or visualizations, they are returned as `image_file` content parts
   - Image file IDs are extracted from message content
   - Images are downloaded from OpenAI using the Files API
   - Images are converted to base64 data URLs for direct browser display
   - Content type (PNG, JPEG, etc.) is automatically detected from file metadata
   - Images are included in the structured response and displayed inline in the UI

### Multi-Assistant Routing Mode

1. **Knowledge Base Setup**: Files are organized by domain:
   - `data/tech/` ‚Üí TechAdvisor vector store
   - `data/marketing/` ‚Üí MarketingAdvisor vector store
   - `data/investor/` ‚Üí InvestorAdvisor vector store
   - Each vector store is isolated to maintain data separation

2. **Assistant Creation**: Three specialized assistants are created:
   - **TechAdvisor**: Technical architecture, AI/ML patterns, system design (code_interpreter enabled)
   - **MarketingAdvisor**: Launch strategy, growth tactics, copywriting (file_search only)
   - **InvestorAdvisor**: Fundraising, KPIs, pitch decks (code_interpreter enabled)
   - Each assistant has domain-specific instructions and scope enforcement

3. **Query Routing**:
   - **Hybrid Routing**: Heuristic keyword matching first, OpenAI classifier if ambiguous
   - Router returns: `{label, confidence, top2_label, margin, is_high_risk}`
   - Routing strategies based on confidence and risk:
     - **Winner-take-all** (confidence ‚â• 0.8): Single assistant responds
     - **Consult-then-decide** (0.5 ‚â§ confidence < 0.8 OR high-risk): Primary assistant answers, then reviewer provides Devil's Advocate critique
     - **Parallel ensemble** (confidence < 0.5 OR margin < 0.15): Top-2 assistants answer independently, both perspectives presented equally
     - **Clarifying question**: If both assistants retrieve nothing, ask user to clarify

4. **Conversation Flow** (Multi-Assistant):
   - User sends a message via web UI or API (streaming or non-streaming)
   - **Flow Detection**: System checks if this is a data analysis flow (file uploads or analysis keywords)
   - **Data Analysis Flow** (if detected):
     - Uses shared analysis thread per session (maintains file context)
     - Files are tracked in `SESSION_ANALYSIS_FILES` per session
     - Follow-up questions automatically re-attach previous files
     - Routes to InvestorAdvisor (has code_interpreter) for data analysis
     - Files persist across all questions in the data flow
   - **Normal Routing Flow** (if not data analysis):
     - **Product Card Extraction**: System automatically extracts product info from message
     - **Deictic Detection**: Identifies context-dependent references ("this", "that", "my product")
     - **Message Rewriting**: If deictic references found, prepends product card context
     - Router classifies query and determines routing strategy
     - Appropriate assistant(s) are selected based on routing logic
     - Each assistant maintains its own conversation thread
     - **Winner-take-all**: Single assistant responds (streams immediately)
     - **Consult-then-decide**: 
       - Primary assistant answers first (streams immediately)
       - Reviewer assistant critiques the response (Devil's Advocate pass)
       - Reviewer response streams and appends to primary (doesn't overwrite)
     - **Parallel ensemble**: Both assistants answer the original question independently
       - Primary response streams first
       - Reviewer response streams and appends when ready
   - Responses are composed differently based on strategy:
     - Consult-then-decide: "Response" + "Critique (Devil's Advocate)"
     - Parallel ensemble: "Perspective" + "Perspective" (equal weight)
   - Sources and images from all assistants are aggregated
   - Structured response includes routing information for debugging
   - **Streaming mode**: All responses stream in real-time with visual indicators

5. **Scope Enforcement**:
   - Each assistant has instructions to defer to others for out-of-scope questions
   - TechAdvisor defers fundraising questions to InvestorAdvisor
   - MarketingAdvisor defers technical questions to TechAdvisor
   - InvestorAdvisor defers marketing questions to MarketingAdvisor
   - This ensures users get the right expertise for their questions

6. **Data Isolation**:
   - Each assistant only searches its own vector store
   - Prevents irrelevant retrievals and maintains meaningful citations
   - Knowledge bases remain separate and focused

7. **Product Card System**:
   - **Auto-Extraction**: Automatically detects product information from user messages
   - **Product Card Structure**: Contains product_id, name, description, target_audience, problem_uvp, key_features, stage, constraints, files
   - **Versioning**: Product cards are versioned and timestamped when updated
   - **Deictic Reference Handling**: When user says "this product" or "my app", product card is automatically injected into the message
   - **Message Rewriting**: User message is rewritten with product context prepended
   - **File Attachment**: Product card files and produced files are automatically attached
   - **Clarification**: If multiple products exist and confidence is low, system asks user to select

8. **File Persistence**:
   - **Data Analysis Flows**: Uses shared analysis thread per session
   - **File Tracking**: Files uploaded to analysis thread are tracked in `SESSION_ANALYSIS_FILES`
   - **Automatic Re-attachment**: Follow-up questions in data flows automatically re-attach previous files
   - **Normal Flows**: Files are tracked per assistant thread in `THREAD_UPLOADED_FILES`
   - **Data Reference Detection**: System detects when user references data ("Q1", "the chart", "best stat")
   - **Context Preservation**: Files persist across multiple turns in the same conversation flow

For detailed information about the routing system, see [MULTI_ASSISTANT_ROUTING.md](MULTI_ASSISTANT_ROUTING.md).

For a comprehensive explanation of how the entire system works, see [SYSTEM_EXPLANATION.md](SYSTEM_EXPLANATION.md).

## Requirements

- `openai==1.51.2` - OpenAI Python SDK
- `fastapi==0.115.5` - Web framework
- `uvicorn[standard]==0.32.0` - ASGI server
- `python-dotenv==1.0.1` - Environment variable management
- `redis==5.0.6` - Redis client for rate limiting
- `fastapi-limiter==0.1.6` - Rate limiting for FastAPI
- `python-multipart==0.0.9` - Form data parsing for file uploads
- `httpx==0.27.2` - HTTP client (dependency)
- `jinja2==3.1.4` - Template engine (dependency)

## License

MIT
