# Founder Copilot (Responses API)

An AI-powered assistant for startup founders, migrated from OpenAI's Assistants API to the Responses API. Core inference runs on `client.responses.create` with optional conversation state via `client.conversations`.

## Migration Status

This project is a migration from the Assistants API to the Responses API. Key changes:

- **Assistants/Threads â†’ Response Config + Conversations**: assistant behavior is represented as response configs; state is kept with conversation IDs.
- **Core generation â†’ Responses API**: requests are executed with `client.responses.create` (streaming and non-streaming paths).
- **Tooling remains OpenAI-native**: `file_search` and `code_interpreter` are passed as Responses tools and schema-validated before execution in workflow paths.
- **Legacy wrapper names retained**: helper names like `create_thread`/`run_assistant_structured` are compatibility wrappers over Responses primitives.

## Features

- ğŸ¤– **AI Assistant** - Powered by GPT-4 with retrieval-augmented generation (RAG)
- ğŸ¯ **Multi-Assistant Routing** - Intelligent routing to specialized assistants (Tech, Marketing, Investor)
- ğŸ“š **Knowledge Base** - Vector store containing startup resources (YC advice, checklists, scenarios)
- ğŸ’¬ **Modern Chat Interface** - Beautiful, responsive web UI with structured responses
- âš¡ **Real-Time Streaming** - Server-Sent Events (SSE) for live response streaming with visual indicators
- ğŸ” **File Search** - Automatic retrieval of relevant information from knowledge base
- ğŸ“ **Source Citations** - Automatic extraction and display of source files with quotes
- ğŸ§® **Code Interpreter** - Python code execution for founder analytics, calculations, and visualizations
- ğŸ“ˆ **Chart Display** - Automatic extraction and display of charts/visualizations generated by code execution
- ğŸ“ **File Upload** - Upload CSV, Excel, or other files for analysis and visualization
- ğŸ“‹ **Product Card System** - Automatic extraction and management of product context for deictic references
- ğŸ”„ **File Persistence** - Automatic re-attachment of files across follow-up questions in data analysis flows
- ğŸ“Š **Metrics Dashboard** - Track token usage, latency (P95), and request statistics
- ğŸš¦ **Rate Limiting** - Redis-based rate limiting to protect API endpoints
- ğŸ³ **Docker Support** - Easy deployment with Docker Compose
- âœ… **Tool Schema Validation** - JSON Schema validation for workflow tool arguments before execution
- ğŸ“¡ **Enterprise Telemetry** - Structured JSON workflow logs with route, tools, tokens, and cost estimate
- ğŸ§ª **Eval Gating** - Offline eval suite and CI workflow regression checks
- ğŸ›Ÿ **Failure Mode Handling** - Single retry on tool failure + timeout partial-warning behavior

## Setup

1. **Create `.env` file**:
   ```bash
   OPENAI_API_KEY=your_api_key_here
   OPENAI_MODEL=gpt-4o
   REDIS_URL=redis://localhost:6379/0
   ```

2. **Build and run with Docker**:
   ```bash
   docker compose up -d --build
   ```

3. **Access the application**:
   - Web Interface: http://localhost:8000
   - Metrics Dashboard: http://localhost:8000/metrics

## Safety + Deployment Architecture

This repo now includes a deployment-grade workflow path at `POST /workflow/execute` that is used for eval/CI:

1. Deterministic route selection (tech/marketing/investor)
2. Tool selection and pre-execution JSON Schema validation
3. Retry-once failure handling for tool execution
4. Timeout fallback with partial warning
5. Structured telemetry log output per workflow:
   - workflow, tenant, route
   - latency, tool count/success, schema validity
   - token counts and simple USD cost estimate

Schema violations are rejected before execution and counted in enterprise metrics.

Deployment workshop + pilot artifacts are in `deployment-playbook/`:
- `deployment-playbook/WORKSHOP_TEMPLATE.md`
- `deployment-playbook/USE_CASE_SCORING_RUBRIC.md`
- `deployment-playbook/PILOT_SUCCESS_METRICS.md`

## Offline Evals

Run evals locally:

```bash
# Terminal 1
uvicorn app.main:app --host 0.0.0.0 --port 8000

# Terminal 2
# compare mode (default): validates behavior + perf regressions vs baseline
python evals/run.py

# update baseline intentionally after approved perf changes
python evals/run.py --update-baseline
# then commit evals/baselines/workflow_baseline.json
```

Dataset location:
- `evals/datasets/workflow_eval.jsonl` (25+ cases)
- Includes routing, tool selection, invalid schema, refusal, and failure mode checks

CI gate:
- `.github/workflows/evals.yml` runs this suite on PRs and pushes to `main`

## Migration Notes

For detailed information about migrating from Assistants API to Responses API, see:
- [OpenAI Migration Guide](https://platform.openai.com/docs/assistants/migration)
- Original project: `founder-copilot` (uses Assistants API)

## File & Image Extraction

This project implements a robust system for extracting and serving files and images from Responses API responses, particularly those generated by code_interpreter.

### Image Extraction Methods

The system extracts images from Responses API responses using multiple strategies:

1. **Direct Output Image Extraction**: Images are extracted from `output_image` content types in `response.output[].content[]`:
   ```python
   # Structure: {"type": "output_image", "image": {"file_id": "file-abc123"}}
   if content_type == "output_image":
       file_id = content_item["image"]["file_id"]
   ```

2. **Container File Citation Extraction**: Images are extracted from `container_file_citation` annotations in text content:
   ```python
   # Annotations contain: {"type": "container_file_citation", "file_id": "...", "container_id": "...", "filename": "chart.png"}
   if ann_type == "container_file_citation" and filename.endswith(('.png', '.jpg', '.jpeg')):
       images.append({"file_id": file_id, "container_id": container_id})
   ```

3. **Fallback Text Parsing**: As a fallback, file IDs are parsed from response text using regex patterns (e.g., `file-abc123` or `sandbox:/mnt/data/file-abc123.png`).

### Container File Retrieval

Container files (images generated by code_interpreter) are retrieved using direct HTTP calls to the Containers API endpoint, as the SDK doesn't support container file downloads:

```python
# Direct HTTP call to: /v1/containers/{container_id}/files/{file_id}/content
url = f"{base_url}/containers/{container_id}/files/{file_id}/content"
req = Request(url)
req.add_header("Authorization", f"Bearer {api_key}")
response = urlopen(req, timeout=60.0)
return response.read()
```

### Image Serving Endpoints

Images are served on-demand via API endpoints:

- **Regular Files**: `GET /api/file/{file_id}` - Downloads files from OpenAI Files API
- **Container Files**: `GET /api/container-file/{container_id}/{file_id}` - Downloads container files via direct HTTP

The frontend receives image metadata (file_id, container_id, url) and displays images by fetching them from these endpoints, avoiding the need to download and store images server-side.

## License

MIT

